# LLMOps Lab 02 - README

## Step 1: Evaluation

[Lab 02 Step 1](./Lab02_Step1_Simple.ipynb)


The purpose of this lab is to guide students through the process of evaluating a large language model (LLM) using key metrics such as context recall, factual correctness, faithfulness, and semantic similarity. These metrics are essential for ensuring the model's outputs align with responsible AI principles, particularly in real-world applications where accuracy, reliability, and ethical considerations are critical. The lab emphasizes the importance of repeatability and transparency in AI evaluations, demonstrating how to standardize the evaluation process using consistent tools and methods. By the end of the lab, students will have a deeper understanding of how to critically assess LLM performance and the importance of applying responsible AI practices when deploying AI systems in sensitive or high-stakes environments.





Credits https://github.com/explodinggradients/ragas