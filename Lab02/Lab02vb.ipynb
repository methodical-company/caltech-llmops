{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b8b88a",
   "metadata": {},
   "source": [
    "\n",
    "# Lab02vb: Deploying PEFTModel on GCP\n",
    "\n",
    "**Objective**: Learn how to fine-tune, containerize, and deploy a Parameter Efficient Fine-Tuning (PEFT) model to Google Cloud Platform (GCP) for production use.\n",
    "\n",
    "## Prerequisites:\n",
    "1. Basic knowledge of Python, Machine Learning, and Docker.\n",
    "2. A Google Cloud Project set up with billing enabled.\n",
    "3. Google Cloud SDK installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3494c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Connect to Google Colab and Configure the Environment\n",
    "# Mount Google Drive to access data and model files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install necessary libraries in Colab\n",
    "!pip install transformers peft torch google-cloud-storage flask gunicorn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3546373",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation:\n",
    "- We use `Google Drive` to store data and models for access within the notebook.\n",
    "- We install necessary packages like `transformers`, `peft`, and `torch` to load and fine-tune models, and `flask` and `gunicorn` to build the API for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Fine-Tune a Hugging Face Model using PEFT\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load a pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Apply a LoRA configuration to the model\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Save the fine-tuned model locally\n",
    "peft_model.save_pretrained(\"/content/drive/MyDrive/peft_model/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e761d71",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation:\n",
    "- `AutoTokenizer` and `AutoModelForCausalLM` load the GPT-2 model and tokenizer.\n",
    "- **PEFT (Parameter Efficient Fine-Tuning)** is used here, specifically **LoRA (Low-Rank Adaptation)**, which adapts the model using fewer parameters.\n",
    "- After fine-tuning, the model is saved to Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Save the Model to Google Cloud Storage (GCS)\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up your Google Cloud Storage bucket name\n",
    "bucket_name = 'your-gcs-bucket-name'\n",
    "\n",
    "def save_model_to_gcs(local_model_dir, bucket_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob('models/peft_model.bin')\n",
    "    blob.upload_from_filename(local_model_dir)\n",
    "\n",
    "save_model_to_gcs('/content/drive/MyDrive/peft_model/pytorch_model.bin', bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0aaed1",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation:\n",
    "- We save the model to Google Cloud Storage (GCS) using the `google-cloud-storage` library.\n",
    "- This step ensures that the model is accessible for further deployment on GCP services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd277dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Build a Flask API to Serve the Model\n",
    "\n",
    "%%writefile app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the fine-tuned PEFTModel from GCS\n",
    "model_name = \"/content/drive/MyDrive/peft_model/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    input_data = request.json[\"input\"]\n",
    "    inputs = tokenizer(input_data, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return jsonify({\"prediction\": prediction})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9001c",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation:\n",
    "- We define a basic Flask API with a `/predict` endpoint.\n",
    "- The model is loaded, and an input string is processed using the tokenizer. The model then generates predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Containerize the API Using Docker\n",
    "\n",
    "%%writefile Dockerfile\n",
    "# Use Python slim image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Install required libraries\n",
    "RUN pip install transformers peft torch flask gunicorn google-cloud-storage\n",
    "\n",
    "# Copy the model and API code\n",
    "COPY ./app.py /app/\n",
    "COPY ./peft_model /app/peft_model/\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Expose the port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Run the app with Gunicorn\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:8080\", \"app:app\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a86c9",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation:\n",
    "- This `Dockerfile` defines a Python 3.9 image and installs all necessary dependencies to serve the model using Flask.\n",
    "- We expose port 8080 for external access and run the app using Gunicorn for production-ready performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Deploy the Container to Google Cloud Run\n",
    "\n",
    "# Step 1: Authenticate with Google Cloud\n",
    "!gcloud auth login\n",
    "\n",
    "# Step 2: Set your Google Cloud Project\n",
    "!gcloud config set project your-project-id\n",
    "\n",
    "# Step 3: Build the Docker image and push to Google Container Registry\n",
    "!gcloud builds submit --tag gcr.io/your-project-id/peftmodel-api .\n",
    "\n",
    "# Step 4: Deploy the Docker container to Cloud Run\n",
    "!gcloud run deploy peftmodel-api     --image gcr.io/your-project-id/peftmodel-api     --platform managed     --region us-central1     --allow-unauthenticated     --memory 4Gi     --cpu 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec2ec1",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation:\n",
    "- First, authenticate with Google Cloud and configure your project.\n",
    "- We use `gcloud builds submit` to package the Docker image and upload it to **Google Container Registry (GCR)**.\n",
    "- The container is deployed to **Google Cloud Run**, a serverless platform that auto-scales with traffic demand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e564e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Testing the Deployed API\n",
    "\n",
    "import requests\n",
    "\n",
    "# Test the API endpoint\n",
    "response = requests.post(\n",
    "    \"https://your-cloud-run-url/predict\",\n",
    "    json={\"input\": \"What is the weather today?\"}\n",
    ")\n",
    "print(response.json())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
