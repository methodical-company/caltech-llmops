{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Installing Required Packages\n",
        "In this step, we install the necessary Python libraries such as `sentence-transformers`, `ragas`, `peft`, and others. These packages are essential for working with language models, evaluation metrics, and datasets throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3410,
          "status": "ok",
          "timestamp": 1729643315406,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "8LN6XY0lfYsz"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers==3.2.1 ragas==0.2.2 peft==0.13.2 bitsandbytes==0.44.1 datasets==3.0.1 wandb==0.18.5 scipy==1.13.1 google-cloud-secret-manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Importing Necessary Libraries\n",
        "Here, we import the necessary libraries and modules. This includes transformers for model handling, LangChain for embedding models, and Ragas for evaluation metrics like answer relevancy and faithfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 7162,
          "status": "ok",
          "timestamp": 1729643322567,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "aFSSbf0afYs0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
        "import torch\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        ")\n",
        "from ragas import evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Accessing Hugging Face Token for Model Authentication\n",
        "In this step, we retrieve the Hugging Face API token, either from the Google Colab environment or via the Google Cloud Secret Manager if the token is not found. This token is required to access and work with models hosted on Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 538,
          "status": "ok",
          "timestamp": 1729643323103,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "zyKdGI3KfYs0",
        "outputId": "149a76c3-6eaa-4324-9267-050780a3e85f"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  userdata.get(\"HF_TOKEN\")\n",
        "except userdata.SecretNotFoundError:\n",
        "  print(\"HuggingFace Token not found, looking in caltech class project\")\n",
        "  from google.cloud import secretmanager\n",
        "  import os\n",
        "  client = secretmanager.SecretManagerServiceClient()\n",
        "  response = client.access_secret_version(request={\"name\": \"projects/240830225929/secrets/HF_TOKEN/versions/1\"})\n",
        "  os.environ[\"HF_TOKEN\"] = response.payload.data.decode(\"UTF-8\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Loading Pre-trained Language Models\n",
        "In this step, we load two pre-trained models from Hugging Face: Model A (SQL fine-tuned) and Model B (a CodeLlama model). We configure both models for quantization to run efficiently on available hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "94c28ff131d44587baf935b861e649c3",
            "55904120acf948d0b61f114e58658a76",
            "becd6f8e2995462c8cd8147999017a53",
            "23d62d3ffb86442e87b74c05cb952ba0",
            "781714ac5b044450a3dca19810be0c25",
            "b4be8fc032b541bb8e8fb135bd33d54e",
            "ce21097fc5d14c3b82850b07d6a3554c",
            "a3297546360a444982bc9f8e99e28aa9",
            "d09512a28c5d421d81831be8ec4ed99a",
            "98207913188947f7babf7c41d6bb45e2",
            "37b7e07ac8664186b161c91f14a91d26",
            "463bd59f9b564e5cbbab2f46cbd58a2b",
            "5f8ea1cfb3d04b5694ba64fe9587faf6",
            "4ee3adaf02e34165b71098f2227f6388",
            "377542d4e1b44cf18a3aff2d7e21e5e0",
            "dc2027e4b7174598ba0d4c10361c32e2",
            "5ebb90f8fc4c40d0ab26c18d5f9d69ac",
            "a8bdb389a44c468680b5fbf0795e3ce6",
            "36883dd94fa74cdfbb7d2d56d6482d69",
            "56bb8379336a43b39c37aab861d92c58",
            "252ca0c02f2640599d8c1ff3c9433312",
            "1d653a8f0e4c4057ae0ba86801bdecec"
          ]
        },
        "executionInfo": {
          "elapsed": 18800,
          "status": "ok",
          "timestamp": 1729643341902,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "GYAnoEJnfYs1",
        "outputId": "2c36ca77-5b8e-4f99-8e83-eac7e78b2381"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Model A From Step 1 in Lab01\n",
        "base_model = \"motherduckdb/DuckDB-NSQL-7B-v0.1\"\n",
        "modelA = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizerA = AutoTokenizer.from_pretrained(base_model)\n",
        "embedding_modelA = HuggingFaceEmbeddings()\n",
        "\n",
        "\n",
        "# Model B From Step 2 in Lab01\n",
        "base_model = \"codellama/CodeLlama-7b-hf\"\n",
        "modelB = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizerB = AutoTokenizer.from_pretrained(base_model)\n",
        "embedding_modelB = HuggingFaceEmbeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Preparing for Multi-GPU Setup (Optional)\n",
        "If multiple GPUs are available, we enable parallel processing for the models. This is useful for distributing the model's workload across GPUs, enhancing computation speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1729643341902,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "zoBai1eIjX3W",
        "outputId": "dd7fc66e-67cf-4337-8a63-59c14b9f5bcb"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"multi cuda devices #{torch.cuda.device_count()}\")\n",
        "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
        "    modelA.is_parallelizable = True\n",
        "    modelA.model_parallel = True\n",
        "    modelB.is_parallelizable = True\n",
        "    modelB.model_parallel = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Loading Evaluation Dataset\n",
        "We load the SQL-related dataset, which includes train and test splits. The test set will be used for evaluating the performance of the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 1423,
          "status": "ok",
          "timestamp": 1729643343324,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "1q8N54hkfYs1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "# train_dataset = dataset.train_test_split(test_size=0.1)[\"train\"]\n",
        "eval_dataset = dataset.train_test_split(test_size=0.1)[\"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Adding Context to the Evaluation Dataset\n",
        "Here, we add a column for 'retrieved_contexts' to the evaluation dataset. This will store the context retrieved for each SQL query during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ae9a6ea222dd46a2bd1c2bb8818493ce",
            "0f23f36247db4c1a8ff31ed72ff384e7",
            "08578c91133f45c1b7b9739cfef014e2",
            "eb04fac8ccc34e828139a2e0efd557fc",
            "724fc05a7c1343f081fde0e807a2df0b",
            "cbf34bc4ff23464b87a351b38b7b6115",
            "ba19311ee1a546f8bf69d8bbe1f940e6",
            "c7873676bd89452892ca7291e4c4acbc",
            "42f204ff349c414084f12c4382750518",
            "8a959b6964694bc7bdce9b06c7447e41",
            "1c201907d8c64908becf7b15d487688e"
          ]
        },
        "executionInfo": {
          "elapsed": 180,
          "status": "ok",
          "timestamp": 1729643343501,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "pc8i6KV3wVSW",
        "outputId": "8f8bd293-2cda-4c05-c6e5-ef1118ce0593"
      },
      "outputs": [],
      "source": [
        "\n",
        "eval_dataset = eval_dataset.add_column(\"retrieved_contexts\", [[],] * len(eval_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Setting Up the Evaluation Pipeline\n",
        "We create a pipeline for evaluating Model A using SQL queries. We configure the evaluation with specific parameters such as maximum token limit, temperature, and repetition penalty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88f3cd46348643b9bae4183beb09e542",
            "ccd4a0968d7b47e88d5ff4f9a5c03847",
            "48960a3f9ad04ccfb8d383124cdfe6c8",
            "548c0549213a42acbae6bebfffd567e4",
            "6bf6ca75555b46b691d57e995891fcc2",
            "82ea251d85ef4574871cb10048b2e1a7",
            "586a1039d032426f81760401b7f594df",
            "b6e958719d2d4c949461f4a12d00e0f6",
            "39bacaef5b95444eada1cf193ef5b45d",
            "b3a348f2fb564ea9ad834ec42c29f715",
            "a299454368bf4c6aae7ddd6faac23ae0"
          ]
        },
        "id": "5MDyKbqzot9u",
        "outputId": "a7da0f25-abfa-4fcf-f2e4-7a7e18878a94"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pipe = pipeline(\n",
        "    model=modelA,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        "    tokenizer=tokenizerA,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")\n",
        "\n",
        "evaluator = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# ragas\n",
        "result = evaluate(\n",
        "    dataset=eval_dataset,\n",
        "    llm=evaluator,\n",
        "    embeddings=embedding_modelA,\n",
        "    metrics=[\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Quiz - Evaluating LLMs for SQL Queries\n",
        "Now that you have completed this exercise, test your understanding with the following quiz questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quiz Questions:\n",
        "1. What is the purpose of the `ragas` library in this notebook?\n",
        "2. Why do we use quantization when loading models?\n",
        "3. What evaluation metrics are applied to assess model performance?\n",
        "4. How do we handle multiple GPUs in the model setup?\n",
        "5. What is the significance of adding the `retrieved_contexts` column in the evaluation dataset?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08578c91133f45c1b7b9739cfef014e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7873676bd89452892ca7291e4c4acbc",
            "max": 7858,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42f204ff349c414084f12c4382750518",
            "value": 7858
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
