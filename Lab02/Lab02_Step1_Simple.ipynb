{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab02: Evaluating a Large Language Model (LLM)\n",
    "In this lab, we will walk through how to evaluate an LLM using various metrics related to responsible AI, repeatability, and other methodologies. We will install necessary libraries, load a dataset, define evaluation metrics, and generate results to assess the model's performance. This exercise emphasizes the importance of responsible AI, focusing on how to validate models for factual correctness, context recall, and faithfulness in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Installing Required Libraries\n",
    "In this step, we install the necessary libraries for the evaluation process. We use `datasets` to load the data, `ragas` for metrics, and `langchain_openai` to integrate OpenAI's model. This installation ensures repeatability by using standardized libraries that can be easily reproduced by others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q datasets ragas langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up API Key\n",
    "Here, we use the `getpass` method to securely input and store the OpenAI API key as an environment variable. This key allows us to access OpenAI's models for evaluation. Using environment variables to store keys is a best practice for security in responsible AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for keyer in (\"OPENAI_API_KEY\", \"HF_TOKEN\"):\n",
    "\n",
    "  if not os.environ.get(keyer):\n",
    "    try:\n",
    "      from google.colab import userdata\n",
    "      userdata.get(keyer)\n",
    "    except userdata.SecretNotFoundError:\n",
    "      print(f\"{keyer} key not found, looking in caltech class project\")\n",
    "      from google.cloud import secretmanager   \n",
    "      client = secretmanager.SecretManagerServiceClient()\n",
    "      response = client.access_secret_version(request={\"name\": f\"projects/240830225929/secrets/{keyer}/versions/1\"})\n",
    "      os.environ[keyer] = response.payload.data.decode(\"UTF-8\")\n",
    "  print(f\"all set with {keyer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Importing Libraries and Modules\n",
    "In this step, we import several key modules for loading data, defining evaluation metrics, and wrapping the LLM. We also import tools to evaluate semantic similarity, context recall, faithfulness, and factual correctness. These metrics are fundamental for assessing the LLM's alignment with responsible AI guidelines, particularly ensuring that the model's output is reliable and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Loading Dataset\n",
    "Here, we load the Amnesty International QA dataset, which will be used for evaluating the LLM. This dataset contains human rights-related questions and answers, and its real-world importance makes it an excellent choice for testing the model's factual correctness and faithfulness. Ensuring that models perform well on such critical datasets aligns with the principles of responsible AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v3\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Preparing Evaluation Dataset\n",
    "We transform the dataset into an evaluation-ready format using `EvaluationDataset.from_hf_dataset()`. This step prepares the dataset for the evaluation process by formatting it in a way that is compatible with the `ragas` evaluation pipeline. This ensures the repeatability of the experiment as others can load the same dataset in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"eval\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Wrapping the LLM and Embeddings\n",
    "In this step, we wrap the LLM (GPT-4) and the embeddings model using Langchain wrappers. Wrapping the models in this manner allows us to integrate them seamlessly into the evaluation process. By clearly defining which LLM and embeddings model we are using, this ensures repeatability and transparency, which are key elements of responsible AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Defining and Applying Evaluation Metrics\n",
    "We define several key metrics for evaluating the model: `LLMContextRecall`, `FactualCorrectness`, `Faithfulness`, and `SemanticSimilarity`. These metrics help ensure that the LLM performs accurately in terms of recalling relevant context, providing factually correct information, staying faithful to the input data, and generating semantically similar results. Applying these metrics is essential for validating models, especially when deploying them in sensitive or high-risk applications where responsible AI principles must be upheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Converting Results to DataFrame\n",
    "Here, we convert the evaluation results into a pandas DataFrame for easy viewing and further analysis. By structuring the results in a DataFrame, we can better visualize and interpret the LLM's performance on different metrics, ensuring a transparent and reproducible evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Displaying the Results\n",
    "Finally, we display the first few rows of the evaluation results to assess the LLM's performance across the defined metrics. This gives us a quick overview of how well the model has performed and whether it meets the desired benchmarks for responsible AI deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Answer the following questions to test your understanding of the evaluation process for LLMs.\n",
    "1. What are the key metrics used to evaluate the LLM in this lab, and why are they important for responsible AI?\n",
    "2. How does the `LangchainLLMWrapper` ensure that the evaluation process is repeatable?\n",
    "3. Why is it important to assess the `FactualCorrectness` of an LLM when deploying it in real-world applications?\n",
    "4. Explain how using environment variables for the API key contributes to responsible AI practices.\n",
    "5. What is the significance of converting the evaluation results into a DataFrame for further analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
