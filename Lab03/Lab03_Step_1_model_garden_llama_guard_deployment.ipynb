{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1dCJPXz9oOl"
      },
      "source": [
        "# Lab 3 Caltech deployment of model on the cloud.\n",
        "\n",
        "In this lab, we will explore the deployment of Llama Guard, a fine-tuned Llama-3.2-1B model designed for content safety classification. This model is particularly well-suited for detecting unsafe or harmful content across different types of inputs, including text prompts and responses from large language models (LLMs). Llama Guard helps maintain content moderation by identifying violations in categories like defamation, elections, and code interpreter abuse, among others. By using this model, we ensure that our AI systems are more responsible, aligning with industry safety standards such as those outlined by MLCommons.\n",
        "\n",
        "The use of Llama Guard in this course is intended to give students hands-on experience with deploying content moderation systems within AI applications. This is particularly useful for real-world scenarios where large language models might generate outputs that require scrutiny for ethical or safety concerns. Deploying a lightweight, fine-tuned model like Llama Guard allows us to introduce safety mechanisms without heavily impacting system performance or requiring significant computational resources.\n",
        "\n",
        "For this class, the focus on content safety is a timely topic, especially as AI tools become more integrated into everyday applications, from customer service to legal document generation. Llama Guard not only demonstrates how AI can be managed responsibly, but also showcases key techniques such as model pruning and quantization, which optimize the deployment of large-scale models on mobile and low-resource devices. This approach balances performance, safety, and accessibility—making it a valuable tool for AI practitioners.\n",
        "\n",
        "Read more about this model here: https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-guard?project=caltech-439204\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTYvGGaoJcrC"
      },
      "source": [
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fmethodical-company%2Fcaltech-llmops%2Frefs%2Fheads%2Fmain%2FLab03%2FLab03_Step_1_model_garden_llama_guard_deployment.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https%3A%2F%2Fraw.githubusercontent.com%2Fmethodical-company%2Fcaltech-llmops%2Frefs%2Fheads%2Fmain%2FLab03%2FLab03_Step_1_model_garden_llama_guard_deployment.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729720330535,
          "user_tz": 420,
          "elapsed": 199,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "#Modified code in complaince with Apache 2 open source\n",
        "# Questions please ask brian@methodical.company\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EED-Xb0GP_IZ"
      },
      "source": [
        "# Vertex AI Model Garden - Llama Guard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and deploying prebuilt [Llama Guard models](https://huggingface.co/meta-llama) with [vLLM](https://github.com/vllm-project/vllm) on GPU, and demonstrates using the Llama Guard model to safeguard LLM inputs and outputs with the Vertex Llama 3 API service.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Download and deploy prebuilt Llama Guard models with [vLLM](https://github.com/vllm-project/vllm) on GPU\n",
        "- Use the Llama Guard models to safeguard LLM inputs and outputs with the Vertex Llama 3.1 API service\n",
        "- Use the Llama Guard models to safeguard LLM vision inputs and outputs with the Vertex Llama 3.2 API service\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 73950,
          "status": "ok",
          "timestamp": 1729720404731,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "9I36hYfmI72D",
        "outputId": "6e7b90e2-35dd-4607-cd3c-29d9c851cca5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/5.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m3.8/5.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'vertex-ai-samples'...\n",
            "remote: Enumerating objects: 41672, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 41672 (delta 30), reused 31 (delta 13), pack-reused 41616 (from 1)\u001b[K\n",
            "Receiving objects: 100% (41672/41672), 96.65 MiB | 27.21 MiB/s, done.\n",
            "Resolving deltas: 100% (32149/32149), done.\n",
            "Enabling Vertex AI API and Compute Engine API.\n",
            "Operation \"operations/acat.p2-240830225929-ca8aabf6-932e-4a3d-9a1e-5180dcd75d2c\" finished successfully.\n",
            "Bucket region us is different from notebook region us-central1\n",
            "Using this GCS Bucket: gs://caltech-class/brian@methodical.company\n",
            "Initializing Vertex AI API.\n",
            "Using this default Service Account: 240830225929-compute@developer.gserviceaccount.com\n",
            "No changes made to gs://caltech-class/\n",
            "Updated property [core/project].\n",
            "Copying Llama Guard model artifacts from gs://vertex-model-garden-public-us/llama3.2 to  gs://caltech-class/brian@methodical.company/llama_guard\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct-meta/consolidated.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/model-00003-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/preprocessor_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/model-00001-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/model-00002-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/chat_template.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/model-00004-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/model-00005-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-Instruct/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-meta/consolidated.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-meta/orig_params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/model-00001-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/model-00002-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/model-00003-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/model-00004-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/model-00005-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/preprocessor_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-11B-Vision/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct-meta/consolidated.00.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-Instruct/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-meta/consolidated.00.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-1B/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct-meta/consolidated.00.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct-meta/orig_params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/model-00001-of-00002.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/model-00002-of-00002.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-Instruct/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B-meta/consolidated.00.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/model-00001-of-00002.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/model-00002-of-00002.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-3B/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.00.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.01.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.02.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.05.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.06.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.03.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.04.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/consolidated.07.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/orig_params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/chat_template.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00001-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00002-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00003-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00004-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00005-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00006-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00007-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00008-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00009-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00014-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00012-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00010-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00011-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00013-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00016-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00015-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00017-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00019-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00018-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00020-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00021-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00022-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00023-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00024-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00025-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00026-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00027-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00028-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00029-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00030-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00031-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00032-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00033-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00036-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00037-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/preprocessor_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00034-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/model-00035-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-Instruct/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.00.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.01.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.02.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.03.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.04.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.05.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.06.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/consolidated.07.pth [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/orig_params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/params.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision-meta/tokenizer.model [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00003-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00002-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00001-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00004-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00005-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00006-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00007-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00008-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00009-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00011-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00010-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00012-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00013-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00014-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00015-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00016-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00017-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00018-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00019-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00020-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00021-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00024-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00022-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00023-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00025-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00026-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00027-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00028-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00029-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00030-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00031-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00032-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00033-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00034-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00035-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00036-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model-00037-of-00037.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-3.2-90B-Vision/preprocessor_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/chat_template.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/model-00001-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/llama_guard_3_11B_vision_figure.png [Content-Type=image/png]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/model-00002-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/model-00003-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/model-00004-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/model-00005-of-00005.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/model.safetensors.index.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/preprocessor_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/tokenizer.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-11B-Vision/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/LICENSE.txt [Content-Type=text/plain]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/README.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/USE_POLICY.md [Content-Type=text/markdown]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/model.safetensors [Content-Type=application/octet-stream]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/generation_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/special_tokens_map.json [Content-Type=application/json]...\n",
            "Copying gs://vertex-model-garden-public-us/llama3.2/Llama-Guard-3-1B/tokenizer.json [Content-Type=application/json]...\n",
            "- [234/234 files][796.1 GiB/796.1 GiB] 100% Done                                \n",
            "Operation completed over 234 objects/796.1 GiB.                                  \n"
          ]
        }
      ],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. Log into your gcp console.\n",
        "\n",
        "username = ! gcloud config get-value account\n",
        "username = username[0]\n",
        "\n",
        "# @markdown 2. {username} will replaced with your username\n",
        "BUCKET_URI =  \"gs://caltech-class/{username}\" # @param {type:\"string\"}\n",
        "BUCKET_URI = BUCKET_URI.format(username=username)\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-west1, europe-west4, asia-southeast1 |\n",
        "\n",
        "# Import the necessary packages\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "# Dedicated endpoint not supported yet\n",
        "use_dedicated_endpoint = False\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        print(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama_guard\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "# @markdown # Access Llama Guard models on Vertex AI\n",
        "# @markdown The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
        "# @markdown Accept the model agreement to access the models:\n",
        "# @markdown 1. Open the [Llama Guard model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-guard) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
        "# @markdown 2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
        "# @markdown 3. After accepting the agreement, a `gs://` URI containing Llama Guard pretrained and finetuned models will be shared.\n",
        "# @markdown 4. Paste the URI in the `VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD` field below.\n",
        "# @markdown 5. The Llama Guard models will be copied into `BUCKET_URI`.\n",
        "\n",
        "\n",
        "VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD = \"gs://vertex-model-garden-public-us/llama3.2\"  # @param {type:\"string\", isTemplate:true}\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD\n",
        "), \"Click the agreement in Vertex AI Model Garden at https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-guard, and get the GCS path of Llama Guard model artifacts.\"\n",
        "parsed_gcs_url = re.search(\"gs://.*?(?=[ ]|$)\", VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD)\n",
        "if parsed_gcs_url:\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD = parsed_gcs_url.group()\n",
        "assert VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD.startswith(\n",
        "    \"gs://\"\n",
        "), \"VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD is expected to be a GCS URI and must start with `gs://`.\"\n",
        "print(\n",
        "    \"Copying Llama Guard model artifacts from\",\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD,\n",
        "    \"to \",\n",
        "    MODEL_BUCKET,\n",
        ")\n",
        "\n",
        "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA_GUARD/* $MODEL_BUCKET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-XybZjtgF9M"
      },
      "source": [
        "## Deploy Llama Guard with vLLM on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E8OiHHNNE_wj",
        "cellView": "form",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729721368492,
          "user_tz": 420,
          "elapsed": 963764,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21063546-33c5-465b-c9f5-81b76a90363a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
            "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/240830225929/locations/us-central1/endpoints/7456079718530416640/operations/8874306403892723712\n",
            "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/240830225929/locations/us-central1/endpoints/7456079718530416640\n",
            "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
            "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/240830225929/locations/us-central1/endpoints/7456079718530416640')\n",
            "INFO:google.cloud.aiplatform.models:Creating Model\n",
            "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/240830225929/locations/us-central1/models/7436785488486203392/operations/3114202480485859328\n",
            "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/240830225929/locations/us-central1/models/7436785488486203392@1\n",
            "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
            "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/240830225929/locations/us-central1/models/7436785488486203392@1')\n",
            "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/240830225929/locations/us-central1/endpoints/7456079718530416640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying llama3-guard-20241023-215343 on g2-standard-8 with 1 NVIDIA_L4 GPU(s).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/240830225929/locations/us-central1/endpoints/7456079718530416640/operations/5990876742468763648\n",
            "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/240830225929/locations/us-central1/endpoints/7456079718530416640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "endpoint_name: 7456079718530416640\n"
          ]
        }
      ],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads prebuilt Llama Guard models to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"Llama-Guard-3-1B\"  # @param [\"Llama-Guard-3-8B\", \"Llama-Guard-3-1B\", \"Llama-Guard-3-11B-Vision\"] {allow-input: true, isTemplate: true}\n",
        "model_id = os.path.join(MODEL_BUCKET, base_model_name)\n",
        "hf_model_id = \"meta-llama/\" + base_model_name\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240925_0916_RC01\"\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "if \"3-1B\" in base_model_name or \"3-8B\" in base_model_name:\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_count = 1\n",
        "    max_num_seqs = 256\n",
        "elif \"3-11B\" in base_model_name:\n",
        "    accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "    machine_type = \"a2-highgpu-1g\"\n",
        "    accelerator_count = 1\n",
        "    max_num_seqs = 12\n",
        "else:\n",
        "    raise ValueError(f\"Recommended GPU setting not found for: {base_model_name}.\")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "gpu_memory_utilization = 0.9\n",
        "max_model_len = 4096\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3-guard\"),\n",
        "    model_id=model_id,\n",
        "    base_model_id=hf_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    enforce_eager=True,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    max_num_seqs=max_num_seqs,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "192a021iB_DE"
      },
      "source": [
        "## Use the Llama Guard models to safeguard LLM inputs and outputs with the Vertex Llama 3.1 API service\n",
        "\n",
        "We use [meta-llama/Llama-Guard-3-8B](https://huggingface.co/meta-llama/Llama-Guard-3-8B) to safeguard input and output conversations with the [Llama 3.1 405B Instruct model API service on Vertex](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.1-405b-instruct-maas).\n",
        "\n",
        "Llama Guard 3 builds on the capabilities introduced with Llama Guard 2, adding three new categories, Defamation, Elections and Code Interpreter Abuse. Additionally this model is multilingual and a new prompt format is introduced, making Llama Guard 3’s prompt format consistent with Llama 3+ Instruct models.\n",
        "\n",
        "This section references [LlamaGuard.ipynb](https://colab.research.google.com/drive/16s0tlCSEDtczjPzdIK3jq0Le5LlnSYGf?usp=sharing) from [https://huggingface.co/meta-llama/LlamaGuard-7b](https://huggingface.co/meta-llama/LlamaGuard-7b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "fHC7INgjB_DF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729721375630,
          "user_tz": 420,
          "elapsed": 7147,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971c1e3a-95cb-48ea-fc40-5b73d27c2185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m378.9/386.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ajjcGNzhB_DF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1729721376761,
          "user_tz": 420,
          "elapsed": 1138,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "import openai\n",
        "\n",
        "# @markdown Set up the Llama 3.1 405B Instruct model API service.\n",
        "\n",
        "# Programmatically get an access token\n",
        "creds, _ = google.auth.default(\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        ")\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "# Note: the credential lives for 1 hour by default (https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi\",\n",
        "    api_key=creds.token,\n",
        ")\n",
        "LLAMA3_405B_INSTRUCT = \"meta/llama-3.1-405b-instruct-maas\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NvSfBcUUB_DF",
        "executionInfo": {
          "status": "error",
          "timestamp": 1729721377412,
          "user_tz": 420,
          "elapsed": 656,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e30ac177-e9bc-408b-ced9-9d25272d1d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation [turn 1]: [{'role': 'user', 'content': 'What is a car?'}]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - [{'error': {'code': 400, 'message': 'Project `240830225929` is not allowed to use Publisher Model `projects/240830225929/locations/us-central1/publishers/meta/models/llama-3.1-405b-instruct-maas`', 'status': 'FAILED_PRECONDITION'}}]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d2e4a010a5f2>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Conversation [turn 1]:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLAMA3_405B_INSTRUCT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    813\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    816\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         )\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - [{'error': {'code': 400, 'message': 'Project `240830225929` is not allowed to use Publisher Model `projects/240830225929/locations/us-central1/publishers/meta/models/llama-3.1-405b-instruct-maas`', 'status': 'FAILED_PRECONDITION'}}]"
          ]
        }
      ],
      "source": [
        "# @markdown Define input message in conversation and get output message from model.\n",
        "\n",
        "message_role = \"user\"  # @param {type: \"string\"}\n",
        "message_content = \"What is a car?\"  # @param {type: \"string\"}\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": message_role,\n",
        "        \"content\": message_content,\n",
        "    }\n",
        "]\n",
        "print(\"Conversation [turn 1]:\", messages)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=LLAMA3_405B_INSTRUCT,\n",
        "    messages=messages,\n",
        ")\n",
        "print(\"Response:\", response)\n",
        "\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": response.choices[0].message.role,\n",
        "        \"content\": response.choices[0].message.content,\n",
        "    }\n",
        ")\n",
        "print(\"Conversation [turn 2]:\", messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7-ym3GlB_DG",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1729721377413,
          "user_tz": 420,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# @markdown Use Llama Guard to classify the conversation: safe versus unsafe.\n",
        "# @markdown Classification is performed on the last turn of the conversation.\n",
        "# @markdown If the content is safe, the model will return `safe`. If the content is unsafe, the model will return `unsafe` and additionally the list of offending categories as a comma-separated list in a new line.\n",
        "# @markdown Set `\"@requestFormat\": \"chatCompletions\"` to use the OpenAI chat completions format.\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"messages\": messages,\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu\"].predict(instances=instances)\n",
        "\n",
        "prediction = response.predictions[0]\n",
        "print(prediction)\n",
        "print(\"Llama Guard prediction:\", prediction[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_LgDtVyO13s"
      },
      "source": [
        "## Use the Llama Guard models to safeguard LLM vision inputs and outputs with the Vertex Llama 3.2 API service\n",
        "\n",
        "We use [meta-llama/Llama-Guard-3-11B-Vision](https://huggingface.co/meta-llama/Llama-Guard-3-11B-Vision) to safeguard input and output conversations with the [Llama 3.2 90B-Vision-Instruct model API service on Vertex](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4hgRrEuqO13s",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1729721377414,
          "user_tz": 420,
          "elapsed": 9,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zw5BkBd4O13s",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1729721377414,
          "user_tz": 420,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "import openai\n",
        "\n",
        "# @markdown Set up the Llama 3.2 90B-Vision-Instruct model API service.\n",
        "\n",
        "# Programmatically get an access token\n",
        "creds, _ = google.auth.default(\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        ")\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "# Note: the credential lives for 1 hour by default (https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi\",\n",
        "    api_key=creds.token,\n",
        ")\n",
        "LLAMA3_90B_VISION_INSTRUCT = \"meta/llama-3.2-90b-vision-instruct-maas\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3xX8VqWFO13s",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1729721377414,
          "user_tz": 420,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# @markdown Define input message in conversation and get output message from model.\n",
        "\n",
        "import base64\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def image_url_to_base64(url):\n",
        "    with urlopen(url) as response:\n",
        "        image_data = response.read()\n",
        "        base64_data = base64.b64encode(image_data).decode(\"ascii\")\n",
        "        return base64_data\n",
        "\n",
        "\n",
        "user_image = \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/The_Blue_Marble_%28remastered%29.jpg/580px-The_Blue_Marble_%28remastered%29.jpg\"  # @param {type: \"string\"}\n",
        "user_message = \"What is in the image?\"  # @param {type: \"string\"}\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": image_url_to_base64(user_image)},\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": user_message},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "print(\"Conversation [turn 1]:\", messages)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=LLAMA3_90B_VISION_INSTRUCT,\n",
        "    messages=messages,\n",
        ")\n",
        "print(\"Response:\", response)\n",
        "\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": response.choices[0].message.role,\n",
        "        \"content\": response.choices[0].message.content,\n",
        "    }\n",
        ")\n",
        "print(\"Conversation [turn 2]:\", messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6zhDnfAcO13s",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1729721377415,
          "user_tz": 420,
          "elapsed": 9,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# @markdown Use Llama Guard to classify the conversation: safe versus unsafe.\n",
        "# @markdown Classification is performed on the last turn of the conversation.\n",
        "# @markdown If the content is safe, the model will return `safe`. If the content is unsafe, the model will return `unsafe` and additionally the list of offending categories as a comma-separated list in a new line.\n",
        "# @markdown Set `\"@requestFormat\": \"chatCompletions\"` to use the OpenAI chat completions format.\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"messages\": messages,\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu\"].predict(instances=instances)\n",
        "\n",
        "prediction = response.predictions[0]\n",
        "print(prediction)\n",
        "print(\"Llama Guard prediction:\", prediction[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "956x4r7rsrza"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1729721377415,
          "user_tz": 420,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab03_Step_1_model_garden_llama_guard_deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "192a021iB_DE"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}