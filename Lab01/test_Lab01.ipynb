{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U peft==0.13.2 bitsandbytes==0.44.1 datasets==3.0.1 wandb==0.18.5 scipy==1.13.1 google-cloud-secret-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq, BitsAndBytesConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  userdata.get(\"HF_TOKEN\")\n",
    "except userdata.SecretNotFoundError:\n",
    "  print(\"HuggingFace Token not found, looking in caltech class project\")\n",
    "  from google.cloud import secretmanager\n",
    "  import os\n",
    "  client = secretmanager.SecretManagerServiceClient()\n",
    "  response = client.access_secret_version(request={\"name\": \"projects/240830225929/secrets/HF_TOKEN/versions/1\"})\n",
    "  os.environ[\"HF_TOKEN\"] = response.payload.data.decode(\"UTF-8\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"multi cuda devices #{torch.cuda.device_count()}\")\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Create a Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# Get the current username\n",
    "username = !gcloud config get-value account\n",
    "username = username[0]\n",
    "\n",
    "# Define the bucket name and the directory to download from\n",
    "bucket_name = 'caltech-class'\n",
    "checkpoint_directory = f'lab02/{username}/checkpoint-400/' # Path in GCS\n",
    "output_dir = 'checkpoint-400/'  # Local directory to save files\n",
    "\n",
    "# Get the GCS bucket\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all files in the GCS directory (checkpoint-80) and download them\n",
    "blobs = bucket.list_blobs(prefix=checkpoint_directory)\n",
    "\n",
    "for blob in blobs:\n",
    "    # Get the relative path of the file within the GCS directory\n",
    "    relative_path = os.path.relpath(blob.name, checkpoint_directory)\n",
    "\n",
    "    # Skip directories, just process files\n",
    "    if not blob.name.endswith('/'):  # Ensures it's a file\n",
    "        # Create any necessary local subdirectories\n",
    "        local_file_path = os.path.join(output_dir, relative_path)\n",
    "        local_subdirectory = os.path.dirname(local_file_path)\n",
    "\n",
    "        if not os.path.exists(local_subdirectory):\n",
    "            os.makedirs(local_subdirectory)\n",
    "\n",
    "        # Download the file\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print(f\"Downloaded {blob.name} to {local_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "### Input:\n",
    "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
